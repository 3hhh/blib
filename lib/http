#!/bin/bash
#
#+Collection of http related functions.
#+
#+Copyright (C) 2018  David Hobach  LGPLv3
#+0.2

b_deps "curl"

#+### Global Variables ###

declare -ga B_HTTP_CHECKURLS
#+B_HTTP_CHECKURLS
#+Each call to [b_http_getOnlineStatus](#b_http_getOnlineStatus) causes one of the URLs in this array to be visited.
#+It is recommended to pick a relatively large number of URLs with SSL support to remain relatively anonymous, if [b_http_getOnlineStatus](#b_http_getOnlineStatus) is called multiple times.
#+Defaults to the European/US Alexa Top 10 (which hopefully blends in to the masses).
B_HTTP_CHECKURLS=(
		"https://www.google.com"
		"https://www.youtube.com"
		"https://www.facebook.com"
		"https://www.wikipedia.org"
		"https://www.yahoo.com"
		"https://www.amazon.com"
		"https://twitter.com"
		"https://www.instagram.com"
		"https://www.reddit.com"
		"https://www.blogger.com"
		)

#+### Functions ###

#+b_http_rawUrlEncode [string]
#+Encode the given string according to RFC 3986.
#+[string]: to encode
#+returns: Returns a string in which all non-alphanumeric characters except -\_.~ have been replaced with a percent (%) sign followed by two hex digits. This is the encoding described in RFC 3986 for protecting literal characters from being interpreted as special URL delimiters, and for protecting URLs from being mangled by transmission media with character conversions (like some email systems). A non-zero exit code is set on errors.
#+@B_E
function b_http_rawUrlEncode {
#original idea from https://stackoverflow.com/questions/296536/how-to-urlencode-data-for-curl-command
#most ideas there don't support non-ASCII characters, but one does it properly:
local str="$1"
local ret=""
ret="$(curl -s -o /dev/null -w %{url_effective} --get --data-urlencode "$str" "")"
[ $? -ne 3 ] && { B_ERR="curl failed to encode $str." ; B_E }
echo -n "${ret:2}"
return 0
}

#+b_http_rawUrlDecode [string]
#+Decode the given string encoded with b_str_rawUrlEncode or an equivalent function.
#+[string]: to decode
#+returns: The literal string with all hex characters replaced; a non-zero exit code is set on errors.
function b_http_rawUrlDecode {
#replace %NN with \xNN and let printf do the hex decoding
printf '%b' "${1//%/\\x}"
}

#+b_http_getOnlineStatus [timeout]
#+Find out whether we are online or not by attempting an http connection.
#+One of [B_HTTP_CHECKURLS](#B_HTTP_CHECKURLS) is possibly visited during the process.
#+[timeout]: Timeout in seconds for hanging checks (default: 5).
#+returns: 0, if we're online, 1 if only DNS works, 2 if neither DNS nor http(s) worked, 3 if the check timed out; [B_E](#B_E) will be called if the status cannot be determined.
#+@B_E
function b_http_getOnlineStatus {
local timeout=${1:-5}
local ind=$(( $RANDOM % ${#B_HTTP_CHECKURLS[@]} ))
local url="${B_HTTP_CHECKURLS[$ind]}"
local ret=
curl -A '' -m $timeout "$url" &> /dev/null
ret=$?

case $ret in
	0)
	return 0
	;;

	1)
	B_ERR="$url is using an unsupported protocol."
	B_E
	;;

	2)
	B_ERR="Failed to initialize curl."
	B_E
	;;

	3)
	B_ERR="$url appears to be malformed."
	B_E
	;;

	4)
	B_ERR="Invalid curl options used."
	B_E
	;;

	5)
	#proxy connection failed
	return 2
	;;

	6)
	#DNS resolution failed
	return 2
	;;

	7)
	#failed to connect (but DNS did work)
	return 1
	;;

	8)
	#weird server reply
	return 0
	;;

	28)
	#timeout from -m --> slow or bad connection
	return 3
	;;

	80)
	#failed to shut down the SSL connection
	return 0
	;;

	*)
	B_ERR="Unexpected curl error code: $ret"
	B_E
	;;
esac
}
